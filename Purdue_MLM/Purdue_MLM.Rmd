---
title: "Mixed models applied to breeding"
author: "Alencar Xavier"
date: "March 13th - March 15th, 2019"
widescreen: true
output: ioslides_presentation
---
## Instructors

**Alencar Xavier**

- Quantitative Geneticist, Corteva Agrisciences
- Adjunct professor, Dpt. of Agronomy, Purdue University

**Luiz Brito**

- Assistant Professor of Animal Sciences, Purdue University

**Hinayah Oliveira**

- Post-doc, Dtp. of Animal Sciences, Purdue University

**Katy Rainey**

- Associate Professor, Dpt. of Agronomy, Purdue University


## Schedule

- Module 1: Intro to mixed models
- Module 2: Fitting mixed models
- Module 3: Advanced topics
- Module 4: Signal detection
- Module 5: Association analysis

# Module 1 - Introduction to mixed models
## Outline

Part 1: Concepts

- History of mixed models
- Mixed models in plant breeding
- Fixed and random terms
- Model notation
- Variance decomposition

Part 2: Applications

- Selection models
- Practical examples
- Variance components
- Ridges and Kernels

## Part 1 - Concepts

## History of mixed models

*Francis Galton* - [1886](http://rspl.royalsocietypublishing.org/content/40/242-245/42): Regression and heritability

*Ronald Fisher* - [1918](https://www.jstor.org/stable/2331838): Infinitesimal model (**P = G + E**)

*Sewall Wright* - [1922](https://www.journals.uchicago.edu/doi/pdfplus/10.1086/279872): Genetic relationship

*Charles Henderson* - [1968](https://www.jstor.org/stable/2528457): BLUP using relationship 

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/geneticists.jpg)

## Mixed models in plant breeding

- *Heart and soul* of plant breeding [(Xavier et al 2017)](https://link.springer.com/article/10.1007/s00122-016-2750-y)
- Variance components and heritability [(Johnson and Thompson 1994)](https://www.journalofdairyscience.org/article/S0022-0302(95)76654-1/abstract)
- Trait associations [(Gianola and Sorensen 2014)](http://www.genetics.org/content/167/3/1407)
- Estimation of genetic and breedingvalues [(Piepho et al 2008)](https://link.springer.com/article/10.1007/s10681-007-9449-8)
- Prediction of unphenotyped lines [(de los Campos et al 2013)](http://www.genetics.org/content/early/2012/06/28/genetics.112.143313)
- Selection index [(Wientjes et al. 2016)](http://www.genetics.org/content/202/2/799.abstract)
- Genome-wide association analysis [(Yang et al 2014)](https://www.nature.com/articles/ng.2876)
- All sorts of inference [(Robinson 1991)](https://projecteuclid.org/download/pdf_1/euclid.ss/1177011926)

## Fixed and random terms

**Fixed effect**

- Assumed to be invariable (often you cannot recollect the data)
- Inferences are made upon the parameters
- Results can not be extrapolated to other datasets
- Example: Overall mean and environmental effects

**Random effects**

- You may not have all the levels available
- Inference are made on variance components
- Prior assumption: coefficients are normally distributed
- Results can not be extrapolated to other datasets
- Example: Genetic effects

## Let's unleash the beast

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/unleash.jpg)

## Model notation

- Linear model: $y=Xb+Zu+e$
- With variance: $y \sim N(Xb,ZKZ\sigma^{2}_{u}+I\sigma^{2}_{e})$

Assuming: $u \sim N(0,K\sigma^{2}_{u})$ and $e \sim N(0,I\sigma^{2}_{e})$

Henderson equation

$$
\left[\begin{array}{rr} X'X & Z'X \\ X'Z & Z'Z+\lambda K^{-1} \end{array}\right] 
\left[\begin{array}{r} b \\ u \end{array}\right]
=\left[\begin{array}{r} X'y \\ Z'y \end{array}\right]
$$
Summary:

- We know (data): $x=\{y,X,Z,K\}$
- We want (parameters): $\theta=\{b,u,\sigma^{2}_{a},\sigma^{2}_{e}\}$
- Estimation based on Gaussian likelihood: $L(x|\theta)$

## Model notation

- **y** = vector of observations (*n*)
- **X** = design matrix of fixed effects (*n* x *p*)
- **Z** = design (or incidence) matrix of random effects (*n* x *q*)
- **K** = random effect correlation matrix (*q* x *q*)
- **u** = vector of random effect coefficients (*q*)
- **b** = vector of fixed effect coefficients (*p*)
- **e** = vector of residuals (*n*)
- $\sigma^{2}_{a}$ = marker effect variance (1)
- $\sigma^{2}_{u}$ = random effect variance (1)
- $\sigma^{2}_{e}$ = residual variance (1)
- $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}$ (Regularization parameters) (1)

## Model notation

The mixed model can also be notated as follows

$$y=Wg+e$$

Solved as

$$[W'W+\Sigma] g = [W'y]$$

Where

$W=[X,Z]$

$g=[b,u]$

$\Sigma = \left[\begin{array}{rr} 0 & 0 \\ 0 & \lambda K^{-1} \end{array}\right]$

## Variance decomposition

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/VarComp.jpg)

## Part 2 - Applications

## Selection

1 *Genetic values*

- BLUPs or BLUEs from replicated trials
- Captures additive and non-additive genetics together

2 *Breeding values*

- Use pedigree information to create $K$
- Captures additive genetics (heritable)
- Trials not necessarily replicated

3 *Genomic Breeding values*

- Genotypic information replaces pedigree
- Any signal: additivity, dominance and epistasis

## Examples

- Example 1: Balanced data, no kinship
- Example 2: Balanced data, with kinship
- Example 3: Unbalanced data, with kinship
- Example 4: Balanced data, missing individual

## Example 1

Data:

```{r,echo=F}
set.seed(1)
ex = data.frame(Env=factor(paste('E',c(1,1,1,1,2,2,2,2,3,3,3,3),sep='')),
                Gen=factor(paste('G',c(1,2,3,4,1,2,3,4,1,2,3,4),sep='')),
                Phe=round(rnorm(12,50,5)))
print(ex)
```

Model: $Phenotype = Environment_{(F)} + Genotype_{(R)}$

## Example 1

Design matrix $W$:

```{r,echo=F}
X = model.matrix(~Env-1,ex)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(~Gen-1,ex))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex$Phe
print(W)
```

## Example 1

$W'W$:

```{r,echo=F}
WW = crossprod(W)
print(WW)
```

## Example 1

Left-hand side ($W'W+\Sigma$):

```{r,echo=F}
WW = crossprod(W)
Sig = diag(c(rep(0,3),rep(0.17,4)))
LHS = WW+Sig;
print(LHS)
```

Assuming independent individuals: $K=I$

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.64/9.56=0.17$

## Example 1

Right-hand side ($W'y$):

```{r,echo=F}
RHS = crossprod(W,y)
print(RHS)
```

## Example 1

We can find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r,echo=F}
g = solve(LHS,RHS)
print(round(g,2))
```

## Shrinkage

$BLUE=\frac{w'y}{w'w}=\frac{sum}{n}$ = *simple average*

$BLUP=\frac{w'y}{w'w+\lambda}=\frac{sum}{n+\lambda}$ = *biased average* = $BLUE\times h^2$

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/Shrinkage.jpg)

**Note**:

- More observations = less shrinkage
- Higher heritability = less shrinkage: $\lambda = \frac{h^{2}-1}{h^2}$


## Example 2

If we know the relationship among individuals:

```{r,echo=F}
set.seed(1)
K = crossprod(matrix(rpois(16,3),4,4))
K = round(K/max(K),2)
diag(K) = 1
colnames(K) = rownames(K) = colnames(Z)
print(K)
```

## Example 2

Then we estimate $\lambda K^{-1}$

```{r,echo=F}
iK_Lmb = round(solve(K)*0.09,2)
print(iK_Lmb)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.64/17.70=0.09$

## Example 2

And the left-hand side becomes

```{r,echo=F}
LHS2 = WW;
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

## Example 2

We can find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r,echo=F}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

Genetic coefficients shrink more: Var(A) < Var(G)

## Example 3

What if we have missing data?

```{r,echo=F}
ex2 = ex
ex2$Phe[c(3,8)] = NA
print(ex2)
```

## Example 3

Rows of missing points are removed

```{r,echo=F}
X = model.matrix(Phe~Env-1,ex2)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(Phe~Gen-1,ex2))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex2$Phe[-c(3,8)]
print(W)
```

## Example 3

$W'W$:

```{r,echo=F}
WW = crossprod(W)
print(WW)
```

## Example 3

Left-hand side ($W'W+\Sigma$):

```{r,echo=F}
LHS2 = WW;
iK_Lmb = round(solve(K)*0.06,2)
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.21/19.61=0.06$

## Example 3 

Right-hand side ($W'y$):

```{r,echo=F}
RHS = crossprod(W,y)
print(RHS)
```

## Example 3

Find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r,echo=F}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

## Example 4

What if we are missing data from a individual?

```{r,echo=F}
ex2 = ex
ex2$Phe[c(1,5,9)] = NA
print(ex2)
```

## Example 4

Rows of missing points are removed

```{r,echo=F}
X = model.matrix(Phe~Env-1,ex2)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(Phe~Gen-1,ex2))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex2$Phe[-c(1,5,9)]
print(W)
```

## Example 4

$W'W$:

```{r,echo=F}
WW = crossprod(W)
print(WW)
```

## Example 4

Left-hand side ($W'W+\Sigma$):

```{r,echo=F}
LHS2 = WW;
iK_Lmb = round(solve(K)*0.08,2)
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.79/22.78=0.08$

## Example 4

Right-hand side ($W'y$):

```{r,echo=F}
RHS = crossprod(W,y)
print(RHS)
```

## Example 4

Find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r,echo=F}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

## Variance components

Expectation-Maximization REML [(1977)](https://www.jstor.org/stable/2286796)

$\sigma^{2}_{u} = \frac{u'K^{-1}u}{q-\lambda tr(K^{-1}C^{22})}$ and $\sigma^{2}_{e} = \frac{e'y}{n-p}$

Bayesian Gibbs Sampling [(1993)](https://gsejournal.biomedcentral.com/track/pdf/10.1186/1297-9686-25-1-41)

$\sigma^{2}_{u} = \frac{u'K^{-1}u+S_u\nu_u}{\chi^2(q+\nu_u)}$ and $\sigma^{2}_{e} = \frac{e'e+S_e\nu_e}{\chi^2(n+\nu_e)}$

Predicted Residual Error Sum of Squares (PRESS) [(2017)](http://www.g3journal.org/content/7/3/895)

- $\lambda = argmin(\sum{e^2_i/(1-h_{ii})^2})$
- Where $H=(I+K\lambda)^{-1}$ and $e=y-\mu-Hy$

## Ridges and Kernels

Kernel methods:

- Genetic signal is captured by the relationship matrix $K$
- Random effect coefficients are the **breeding values** (BV)
- Efficient to compute BV when $markers \gg individuals$
- Easy use and combine pedigree, markers and interactions

Ridge methods:

- Genetic signal is captured by the design matrix $M$
- Random effect coefficients are the **marker effects**
- Easy way to make predictions of unobserved individuals
- Enables to visualize where the QTLs are in the genome

## Ridges and Kernels

Kernel

$y=Xb+Zu+e$, $u\sim N(0,K\sigma^2_u)$

Ridge

$y=Xb+Ma+e$, $a\sim N(0,I\sigma^2_a)$

Where

- $M$ is the genotypic matrix, $m_{ij}=\{0,1,2\}$
- $K=\alpha MM'$
- $u=Ma$
- $\sigma^2_a=\alpha\sigma^2_u$

## Ridges and Kernels

Kernel model

$$
\left[\begin{array}{rr} X'X & Z'X \\ X'Z & Z'Z+K^{-1}(\sigma^2_e/\sigma^2_u) \end{array}\right] 
\left[\begin{array}{r} b \\ u \end{array}\right]
=\left[\begin{array}{r} X'y \\ Z'y \end{array}\right]
$$

Ridge model

$$
\left[\begin{array}{rr} X'X & M'X \\ X'M & M'M+I^{-1}(\sigma^2_e/\sigma^2_a) \end{array}\right] 
\left[\begin{array}{r} b \\ a \end{array}\right]
=\left[\begin{array}{r} X'y \\ M'y \end{array}\right]
$$

Both models capture same genetic signal [(de los Campos 2015)](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1005048)

## Ridges and Kernels

```{r,echo=F}
require(NAM,quietly = TRUE)
data(tpod)
M = gen
par(mar=c(4,4,0,0))
```

```{r,echo=TRUE,fig.width=5,fig.height=4,fig.align="center"}
K = tcrossprod(M)/ncol(M)
GBLUP = reml(y=y,K=K); Kernel_GEBV = GBLUP$EBV
RRBLUP = reml(y=y,Z=M); Ridge_GEBV = M%*%RRBLUP$EBV
plot(Kernel_GEBV,Ridge_GEBV, main='Comparing results')
```

# Break
# Module 2 - Fitting mixed models

## Example 1 - Sorghum

## Example 1 - load data

Example dataset from Kevin's *agridat* package

```{r,echo=TRUE}
data(adugna.sorghum, package = 'agridat')
dt = adugna.sorghum
head(dt)
```

## Example 1 - Getting design matrix

- Linear model: $Pheotype = Env + Gen$
- In algebra notation: $y = Xb + Zu + e$

```{r,echo=TRUE}
y = dt$yield
X = model.matrix(y~env,dt)
Z = model.matrix(y~gen-1,dt) # "-1" means no intercept
```

Assuming:

- $u\sim N(0,I\sigma^2_g)$
- $e\sim N(0,I\sigma^2_e)$

## Example 1 - Visualize X and Z matrices

```{r,echo=T,fig.width=8,fig.height=4,fig.align="center"}
SEE=function(A,...)image(t(1-A[nrow(A):1,]),axes=F,col=gray.colors(2),...)
par(mfrow=c(1,2),mar=c(1,1,3,1))
SEE(X, main=paste("X matrix (",paste(dim(X),collapse=' x '),")" ))
SEE(Z, main=paste("Z matrix (",paste(dim(Z),collapse=' x '),")" ))
```

## Example 1 - Fit the model

```{r,echo=TRUE}
# Using the NAM package (same for rrBLUP, EMMREML, BGLR)
require(NAM, quietly = TRUE)
fit1 = reml(y=y,X=X,Z=Z)
# Alternatively, you can also use formulas with NAM
fit1b = reml(y=dt$yield,X=~dt$env,Z=~dt$gen )
# Using the lme4 package
require(lme4, quietly = TRUE)
fit2 = lmer(yield ~ env + (1|gen), data=dt)
```

## Example 1 - Variance components

```{r,echo=TRUE}
fit1$VC[c(1:2)] # same with fit1b$VC
data.frame((summary(fit2))$varcor)$vcov
```

- VC can be used to measure *broad-sense* heritability

$H=\frac{\sigma^2_g}{\sigma^2_g+\sigma^2_e/n}=\frac{189680.4}{189680.4+442075.6/10.32}=0.82$

## Example 1 - The coefficients

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
blue1 = fit1$Fixed[,1];  blup1 = fit1$EBV
blue2 = fit2@beta;       blup2 = rowMeans(ranef(fit2)$gen)
par(mfrow=c(1,2));
plot(blue1,blue2,main="BLUE"); plot(blup1,blup2,main="BLUP")
```

## Example 1 - DIY BLUPs

```{r,echo=TRUE}
iK = diag(ncol(Z))
Lambda = 442075.6/189680.4
W = cbind(X,Z)
Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
LHS = crossprod(W) + Sigma
RHS = crossprod(W,y)
g = solve(LHS,RHS)
my_blue = g[ c(1:ncol(X))]
my_blup = g[-c(1:ncol(X))]
```

## Example 1 - DIY BLUPs

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
par(mfrow=c(1,2))
plot(my_blue,blue1,main="BLUE")
plot(my_blup,blup1,main="BLUP")
```

## Example 1 - DIY Variance components

$\sigma^{2}_{e} = \frac{e'y}{n-p}$ and $\sigma^{2}_{u} = \frac{u'K^{-1}u+tr(K^{-1}C^{22}\sigma^{2}_{e})}{q}$

```{r,echo=TRUE}
e = y - X %*% my_blue - Z %*% my_blup
Ve = c(y%*%e)/(length(y)-ncol(X))
Ve
trKC22 = sum(diag(iK%*%(solve(LHS)[-c(1:ncol(X)),-c(1:ncol(X))])))
Vg = Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
Vg
```


## Starting from bad variance components

```{r,echo=TRUE}
Ve = Vg = 1
for(i in 1:25){
  Lambda = Ve/Vg; 
  Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
  LHS = crossprod(W) + Sigma; RHS = crossprod(W,y); g = solve(LHS,RHS)
  my_blue = g[ c(1:ncol(X))]; my_blup = g[-c(1:ncol(X))]
  e = y - X%*%my_blue - Z%*%my_blup; Ve = c(y%*%e)/(length(y)-ncol(X))
  trKC22 = sum(diag(iK%*%(solve(LHS)[(ncol(X)+1):(ncol(W)),(ncol(X)+1):(ncol(W))])))
  Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
  if(!i%%5){cat('It',i,'VC:  Vg =',Vg,'and Ve =',Ve,'\n')}}
```

## Example 2 - Barley

## Example 2 - load data

Another example dataset from Kevin's *agridat* package

```{r,echo=TRUE}
data(steptoe.morex.pheno,package='agridat')
dt = steptoe.morex.pheno
head(dt)
```

## Example 2 - Getting design matrix

- Linear model: $Phe=Env+Gen$

- In algebra notation: $y=Xb+Zu+e$  

```{r,echo=TRUE}
X = model.matrix(~env,dt)
Z = model.matrix(~gen-1,dt) # "-1" means no intercept
y = dt$yield
```

## Example 2 - Fit the model

```{r,echo=TRUE}
# Fit
fit0 = reml(y=y,X=X,Z=Z)
# BLUE and BLUP
blue0 = fit0$Fixed[,1]
blup0 = fit0$EBV
# Get VC
fit0$VC[c(1:2)]
```

## Example 2 - DIY BLUPs

```{r,echo=TRUE}
iK = diag(ncol(Z))
Lambda = 0.637997/0.132009
W = cbind(X,Z)
Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
LHS = crossprod(W) + Sigma
RHS = crossprod(W,y)
g = solve(LHS,RHS)
my_blue = g[ c(1:ncol(X))]
my_blup = g[-c(1:ncol(X))]
```

## Example 2 - DIY BLUPs

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
par(mfrow=c(1,2))
plot(my_blue,blue0,main="BLUE")
plot(my_blup,blup0,main="BLUP")
```

## Example 2 - Check variance components

$\sigma^{2}_{e} = \frac{e'y}{n-p}$ and $\sigma^{2}_{u} = \frac{u'K^{-1}u+tr(K^{-1}C^{22}\sigma^{2}_{e})}{q}$

```{r,echo=TRUE}
e = y - X %*% my_blue - Z %*% my_blup
Ve = c(y%*%e)/(length(y)-ncol(X))
Ve
trKC22 = sum(diag(iK%*%(solve(LHS)[-c(1:ncol(X)),-c(1:ncol(X))])))
Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
Vg
```

## Starting from bad variance components

```{r,echo=TRUE}
Ve = Vg = 1
for(i in 1:25){
  Lambda = Ve/Vg; 
  Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
  LHS = crossprod(W) + Sigma; RHS = crossprod(W,y); g = solve(LHS,RHS)
  my_blue = g[ c(1:ncol(X))]; my_blup = g[-c(1:ncol(X))]
  e = y - X%*%my_blue - Z%*%my_blup; Ve = c(y%*%e)/(length(y)-ncol(X))
  trKC22 = sum(diag(iK%*%(solve(LHS)[(ncol(X)+1):(ncol(W)),(ncol(X)+1):(ncol(W))])))
  Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
  if(!i%%5){cat('It',i,'VC:  Vg =',Vg,'and Ve =',Ve,'\n')}}
```

## Example 3 - Barley GEBV

## Using genomic information!

```{r,echo=TRUE}
data(steptoe.morex.geno,package='agridat')
gen = do.call("cbind",lapply(steptoe.morex.geno$geno,function(x) x$data))
gen = rbind(0,2,gen)
rownames(gen) = c('Morex','Steptoe',as.character(steptoe.morex.geno$pheno$gen))
rownames(gen)[10] = "SM8"
gen = gen[gsub('gen','',colnames(Z)),]
K = GRM(IMP(gen),T)
```

## Example 3 - Fit the model

```{r,echo=TRUE}
# Fit model
fit0 = reml(y=y,X=X,Z=Z,K=K)
# BLUE and BLUP
blue0 = fit0$Fixed[,1]
gebv0 = fit0$EBV
# Get VC
fit0$VC[c(1:2)]
```

## Example 3 - DIY BLUPs

```{r,echo=TRUE}
diag(K) = diag(K)+1e-08; iK = solve(K)
Lambda = 0.6575/1.0280
W = cbind(X,Z)
Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
LHS = crossprod(W) + Sigma
RHS = crossprod(W,y)
g = solve(LHS,RHS)
my_blue = g[ c(1:ncol(X))]
my_gebv = g[-c(1:ncol(X))]
```

## Example 3 - DIY BLUPs

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
par(mfrow=c(1,2))
plot(my_blue,blue0,main="BLUE")
plot(my_gebv,gebv0,main="GEBV")
```

## Example 3 - Check variance components

$\sigma^{2}_{e} = \frac{e'y}{n-p}$ and $\sigma^{2}_{u} = \frac{u'K^{-1}u+tr(K^{-1}C^{22}\sigma^{2}_{e})}{q}$

```{r,echo=TRUE}
e = y - X %*% my_blue - Z %*% my_blup
Ve = c(y%*%e)/(length(y)-ncol(X))
Ve
trKC22 = sum(diag(iK%*%(solve(LHS)[(ncol(X)+1):(ncol(W)),(ncol(X)+1):(ncol(W))])))
Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
Vg
```

## Example 4 - Soybeans

## snp-BLUP

```{r,echo=TRUE}
data(tpod,package='NAM')
X = matrix(1,length(y),1)
Z = gen
dim(Z)
```

## Example 4 - Fit the model

```{r,echo=TRUE,fig.width=8,fig.height=3.5,fig.align="center"}
# Fit using the lme4 package
fit0 = reml(y=y,X=X,Z=Z) # same as reml(y=y,Z=gen) 
marker_values = fit0$EBV
gebv0 = c(gen %*% marker_values)
# Marker effects
plot(marker_values,pch=16, xlab='SNP')
```

## Example 4 - DIY BLUPs

```{r,echo=TRUE}
iK = diag(ncol(Z))
Lambda = fit0$VC[2] / fit0$VC[1]
W = cbind(X,Z)
Sigma = diag( c(0,rep(Lambda,ncol(Z))) )
LHS = crossprod(W) + Sigma
RHS = crossprod(W,y)
g = solve(LHS,RHS)
my_mu = g[ c(1:ncol(X))]
my_marker_values = g[-c(1:ncol(X))]
my_gebv = c(gen %*% my_marker_values) # GEBVs from RR
```

## Example 4 - DIY BLUPs

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
par(mfrow=c(1,2))
plot(my_marker_values, marker_values, main="Markers")
plot(my_gebv, gebv0, main="GEBV")
```

## Example 4 - Heritability from RR

```{r,echo=TRUE,fig.width=4,fig.height=4,fig.align="center"}
fit0$VC
Scale=sum(apply(gen,2,var)); Va=fit0$VC[1]*Scale;  Ve=fit0$VC[2]
round((Va/(Va+Ve)),2)
K = tcrossprod(apply(gen,2,function(x) x-mean(X)))
K = K/mean(diag(K)); round(reml(y,K=K)$VC,2)
```

## Estimate VC from bad starters

```{r,echo=TRUE}
W = cbind(X,Z); iK = diag(ncol(Z))
Ve = Vg = 1 # Bad starting values
for(i in 1:100){ # Check the VC convergence after few iterations
  Lambda = Ve/Vg; 
  Sigma = as.matrix(Matrix::bdiag(diag(0,ncol(X)),iK*Lambda))
  LHS = crossprod(W) + Sigma; RHS = crossprod(W,y); g = solve(LHS,RHS)
  my_blue = g[ c(1:ncol(X))]; my_blup = g[-c(1:ncol(X))]
  e = y - X%*%my_blue - Z%*%my_blup; Ve = c(y%*%e)/(length(y)-ncol(X))
  trKC22 = sum(diag(iK%*%(solve(LHS)[(ncol(X)+1):(ncol(W)),(ncol(X)+1):(ncol(W))])))
  Vg = c(t(my_blup)%*%iK%*%my_blup+trKC22*Ve)/ncol(Z)
  if(!i%%25){cat('It',i,'VC:  Vg =',Vg,'and Ve =',Ve,'\n')}}
```

# Break

# Module 3 - Advanced topics

## Outline

- Multivariate models
- Bayesian methods
- Machine learning
- G x E interactions

## Multivariate models

![](http://www.genetics.org/content/genetics/194/3/561/F1.large.jpg)

## Multivariate models

Mixed models also enable us to evaluate multiple traits:

* More accurate parameters: BV and variance components
* Information: Inform how traits relate to each other
* Constrains: May increase computation time considerably

It preserves the same formulation

$$y = Xb+Zu+e$$
However, we now stack the traits together:

$y=\{y_1,y_2,...,y_k\}$, $X=\{X_1, X_2, ... , X_k\}'$, $b=\{b_1,b_2,...,b_k\}$, $Z=\{Z_1, Z_2, ... , Z_k\}'$, $u=\{u_1,u_2,...,u_k\}$, $e=\{e_1,e_2,...,e_k\}$.

## Multivariate models

The multivariate variance looks nice at first
$$
Var(y) = Var(u) + Var(e)
$$

But can get ugly with a closer look:

$$
Var(u) = Z ( G 	\otimes \Sigma_a ) Z' = 
\left[\begin{array}{rr} Z_1 G Z_1'\sigma^2_{a_1} & Z_1 G Z_2'\sigma_{a_{12}}
\\ Z_2 G Z_1' \sigma_{a_{21}} & Z_2 G Z_2'\sigma^2_{a_2} \end{array}\right] 
$$

and

$$
Var(e) = R	\otimes \Sigma_e = 
\left[\begin{array}{rr} R\sigma^2_{e_1} & R\sigma_{e_1e_2}
\\ R\sigma_{e_2e_1} & R\sigma^2_{e_2} \end{array}\right] 
$$

## Multivariate models

You can still think the multivariate mixed model as

$$y=Wg+e$$

Where

$$
y = \left[\begin{array}{rr} y_1 \\ y_2 \end{array}\right],
W = \left[\begin{array}{rr} X_1 & 0 & Z_1 & 0 \\ 0 & X_2 & 0 & Z_2 \\ \end{array}\right],
g = \left[\begin{array}{rr} b_1 \\ b_2 \\ u_1 \\ u_2 \end{array}\right],
e = \left[\begin{array}{rr} e_1 \\ e_2 \end{array}\right]
$$

## Multivariate models

Left-hand side ($W'R^{-1}W+\Sigma$)
$$ 
\left[\begin{array}{rr}
X_1'X_1\Sigma^{-1}_{e_{11}} & X_1'X_2\Sigma^{-1}_{e_{12}} & X_1'Z_1\Sigma^{-1}_{e_{11}} & X_1'Z_2\Sigma^{-1}_{e_{12}}\\
X_2'X_1\Sigma^{-1}_{e_{12}} & X_2'X_2\Sigma^{-1}_{e_{22}} & X_2'Z_1\Sigma^{-1}_{e_{12}} & X_2'Z_2\Sigma^{-1}_{e_{22}}\\
Z_1'X_1\Sigma^{-1}_{e_{11}} & Z_1'X_2\Sigma^{-1}_{e_{12}} & G^{-1}\Sigma^{-1}_{a_{11}} + Z_1'Z_1\Sigma^{-1}_{e_{11}} & G^{-1}\Sigma^{-1}_{a_{12}} + Z_1'Z_2\Sigma^{-1}_{e_{12}}\\
Z_2'X_1\Sigma^{-1}_{e_{12}} & Z_2'X_2\Sigma^{-1}_{e_{22}} & G^{-1}\Sigma^{-1}_{a_{11}} + Z_2'Z_1\Sigma^{-1}_{e_{12}} & G^{-1}\Sigma^{-1}_{a_{22}} + Z_2'Z_2\Sigma^{-1}_{e_{22}}\\ 
\end{array}\right]
$$
Right-hand side ($W'R^{-1}y$)
$$
\left[\begin{array}{rr}
X_1'(y_1\Sigma^{-1}_{e_1}+y_2\Sigma^{-1}_{e_12}) \\
X_2'(y_1\Sigma^{-1}_{e_22}+y_2\Sigma^{-1}_{e_12}) \\
Z_1'(y_1\Sigma^{-1}_{e_12}+y_2\Sigma^{-1}_{e_1}) \\
Z_2'(y_1\Sigma^{-1}_{e_12}+y_2\Sigma^{-1}_{e_12}) \\
\end{array}\right]
$$

## Multivariate models

```{r,echo=T,fig.width=8,fig.height=4,fig.align="center"}
data(wheat, package = 'BGLR')
G = NAM::GRM(wheat.X);
Y = wheat.Y; colnames(Y) = c('E1','E2','E3','E4')
mmm = NAM::reml( y = Y, K = G )
knitr::kable( round(mmm$VC$GenCor,2) )
```

## Multivariate models

```{r,echo=TRUE}
mmm$VC$Vg
mmm$VC$Ve
```

## Multivariate models

- Selection indeces, co-heritability, indirect response to selection
- Study residual and additive genetic association among traits

```{r,echo=TRUE,fig.width=6,fig.height=4,fig.align="center"}
biplot(princomp(mmm$VC$GenCor,cor=T),xlim=c(-.4,.4),ylim=c(-.11,.11))
```

## Multivariate models

When do additional traits contribute?

```{r,echo=TRUE}
# Fit E4, predict E2
fit_E4=bWGR::mrr(matrix(Y[,4]),wheat.X); cor(fit_E4$hat[,1],Y[,2])
# Fit E4 and E1, E4 predict E2
fit_E4E1=bWGR::mrr(Y[,c(1,4)],wheat.X); cor(fit_E4E1$hat[,2],Y[,2])
# Fit E4 and E3, E4 predict E2
fit_E4E3=bWGR::mrr(Y[,3:4],wheat.X); cor(fit_E4E3$hat[,2], Y[,2])
```



## Bayesian methods

![](https://www2.isye.gatech.edu/~brani/isyebayes/bank/bayesfun.jpg)

## Bayesian methods

The general framework on a hierarchical Bayesian model follows:

$$p(\theta|x)\propto p(x|\theta) p(\theta)$$

Where:

* Posterior probability: $p(\theta|x)$
* Likelihood: $p(x|\theta)$
* Prior probability: $p(\theta)$

## Bayesian methods

For the model:

$$y=Xb+Zu+e, \space\space\space u\sim N(0,K\sigma^2_a), \space e \sim N(0,I\sigma^2_e)$$

* Data ($x=\{y,X,Z,K\}$)
* Parameters ($\theta=\{b,u,\sigma^2_a,\sigma^2_e\}$)

Probabilistic model:

$$p(b,u,\sigma^2_a,\sigma^2_e|y,X,Z,K) \propto N(y,X,Z,K|b,u,\sigma^2_a,\sigma^2_e) \times \space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space
\space\space\space\space\space\space \\ N(b, u|\sigma^2_a,\sigma^2_e) \times  \chi^{-2}(\sigma^2_a,\sigma^2_e|S_a,S_e,\nu_a,\nu_e)$$

## Bayesian methods

REML: the priors $(S_a,S_e,\nu_a,\nu_e)$ are estimated from data.

Hierarchical Bayes: You provide priors. Here is how:

$$\sigma^2_a=\frac {u'K^{-1}u+S_a\nu_a} {\chi^2(q+\nu_a)}$$

`sigma2a=(t(u)%*%iK%*%u+Sa*dfa)/rchisq(df=ncol(Z)+dfa,n=1)`

$$\sigma^2_e=\frac {e'e+S_e\nu_e} {\chi^2(n+\nu_e)}$$

`sigma2e=(t(e)%*%e+Se*dfe)/rchisq(df=length(y)+dfe,n=1)`

## Bayesian methods

What does it mean for **you**? If your "prior knowledge" tells you that a given trait has approximately $h^2=0.5$ (nothing unreasonable). In which case, half of the phenotypic variance is due to genetics, and the other half is due to error. Your prior shape is:

$$S_a = S_e= \sigma^2_y\times0.5$$

We usually assign small a prior degrees of freeds. Samething like four or five prior degrees of freedom. That means that assuming $\nu_0=5$, you are yielding to your model 5 data points that support heritability 0.5 

$$\nu_a=\nu_e=5$$

Example of prior influence: In a dataset with 300 data points, 1.6% of the variance components information comes from prior (5/305), and 98.4% comes from data (300/305).


## Bayesian methods

For whole-genome regression models

$$y=\mu+Ma+e, \space\space\space a\sim N(0,I\sigma^2_b), \space e\sim N(0,I\sigma^2_e)$$

We scale the prior genetic variance based on allele frequencies

$$ S_b = \frac{\sigma^2_y\times0.5}{2 \sum{p_j(1-p_j)} }$$

Two common settings:

* All markers, one random effect: *Bayesian ridge regression*
* Each markers as a random effect: *BayesA*


## Machine learning methods

* Parametric methods for prediction: L1-L2
* Semi-parametric methods for prediction: Kernels
* Non-parametric methods for prediction: Trees and nets

![](https://cdn-images-1.medium.com/max/1600/0*SbLLq8PDBbxkMO6X.png)

## Machine learning methods

L1-L2 machines include all mixed and Bayesian models we have seen so far. The basic framework is driven by a single (random) term model:

$$y=Xb+e$$

The univariate soltion indicates how the model is solved. A model without regularization yields the least square (LS) solution. If we regularize by deflating the nominator, we get the L1 regularization (LASSO). If we regularize by inflating the denominator, we get the L2 regularization (Ridge). For any combination of both, we get a elastic-net (EN). Thus:

$$b_{LS}=\frac{x'y}{x'x}, \space\space b_{Lasso}=\frac{x'y-\lambda}{x'x}, \space\space b_{Ridge}=\frac{x'y}{x'x+\lambda}, \space\space b_{EN}=\frac{x'y-\lambda_1}{x'x+\lambda_2}$$

Whereas the Bayesian and mixed model framework resolves the regularization as $\lambda=\sigma^2_e/\sigma^2_b$, ML methods search for $\lambda$ through (*k*-fold) cross-validation.

## Machine learning methods

Common loss functions in L1-L2 machines

* LS (no prior, no shrinkage): $argmin( \sum{e_i^2} )$
* L1 (Laplace prior with variable selection): $argmin( \sum{e_i^2} + \lambda\sum{|b_j|} )$
* L2 (Gaussian prior, unique solution): $argmin( \sum{e_i^2} + \lambda\sum{b^2_j} )$

Other losses that are less popular

* Least absolute: $argmin( \sum{|e_i|} )$ based on $b_{LA}=\frac{ n MD(x\times y)}{x'x}$
* $\epsilon$-loss: $argmin(\sum{e_i^2, |e_i|>\epsilon})$ - used in support vector machines

## Machine learning methods

Cross-validations to search for best value of lambda

```{r,echo=TRUE,fig.width=7,fig.height=3,fig.align="center"}
lasso = glmnet::cv.glmnet(x=wheat.X,y=rowMeans(Y),alpha=1);
ridge = glmnet::cv.glmnet(x=wheat.X,y=rowMeans(Y),alpha=0);
par(mfrow=c(1,2)); plot(ridge); plot(lasso)
```

## Machine learning methods

Re-fit the model using this best value

```{r,echo=TRUE,fig.width=7,fig.height=3,fig.align="center"}
lasso = glmnet::glmnet(x=wheat.X,y=rowMeans(Y),lambda=lasso$lambda.min,alpha=1)
ridge = glmnet::glmnet(x=wheat.X,y=rowMeans(Y),lambda=ridge$lambda.min,alpha=0)
par(mfrow=c(1,2)); plot(lasso$beta,main='LASSO'); plot(ridge$beta,main='RIDGE');
```

## Machine learning methods

Of course, the losses presented above are not limited to the application of prediction and classification. Below, we see an example of deploying LASSO for a graphical model (Markov Random Field): How the traits of the multivariate model relate in terms of additive genetics:

```{r,echo=TRUE,fig.width=6,fig.height=3,fig.align="center"}
ADJ=huge::huge(mmm$VC$GenCor,.3,method='glasso',verbose=F)$path[[1]]
plot(igraph::graph.adjacency(adjmatrix=ADJ),vertex.label.cex=3)
```

## Machine learning methods

Reproducing kernel Hilbert Spaces (RKHS), is a generalization of a GBLUP... Most commonly instead of using the linear kernel ($ZZ'\alpha$), RKHS commonly uses one or more Gaussian or exponential kernels:

$$K=\exp(-\theta D^2)$$

Where $D^2$ is the squared Euclidean distance, and $\theta$ is a bandwidth:

* Single kernel: $1/mean(D^2)$
* Three kernels: $\theta$={5/q, 1/q, 0.2/q}, where `q=quantile(D2,0.05)`

## Machine learning methods

We can use REML, PRESS (=cross-validation) or Bayesian approach to solve RKHS

```{r,echo=TRUE,fig.width=3,fig.height=3,fig.align="center"}
# Make the kernel
D2 = as.matrix(dist(wheat.X)^2)
K = exp(-D2/mean(D2))
# Below we are going to calibrate models on Env 2 and predict Env 3
rkhs_press = NAM::press(y=Y[,2],K=K)$hat
rkhs_reml = NAM::reml(y=Y[,2],K=K)$EBV
rkhs_bgs = NAM::gibbs(y=Y[,2],iK=solve(K))$Fit.mean
```

## Machine learning methods

```{r,echo=TRUE,fig.width=8,fig.height=3,fig.align="center"}
par(mfrow=c(1,3))
plot(rkhs_press,Y[,3],main=paste('PRESS, cor =',round(cor(rkhs_press,Y[,3]),4) ))
plot(rkhs_reml,Y[,3],main=paste('REML, cor =',round(cor(rkhs_reml,Y[,3]),4) ))
plot(rkhs_bgs,Y[,3],main=paste('Bayes, cor =',round(cor(rkhs_bgs,Y[,3]),4) ))
```

## Machine learning methods

RKHS for epistasis and variance component analysis

```{r,echo=TRUE,fig.width=3.5,fig.height=3.5,fig.align="center"}
Ks = NAM::G2A_Kernels(wheat.X) # Get all sorts of linear kernels
FIT = BGLR::BGLR(rowMeans(Y),verbose=FALSE,
      ETA=list(A=list(K=Ks$A,model='RKHS'),AA=list(K=Ks$A,model='RKHS')))
pie(c(Va=FIT$ETA$A$varU,Vaa=FIT$ETA$AA$varU,Ve=FIT$varE),main='Epistasis')
```

## Machine learning methods

For the same task (E2 predict E3), let's check members of the Bayesian alphabet

```{r,echo=TRUE}
fit_BRR = bWGR::wgr(Y[,2],wheat.X); cor(c(fit_BRR$hat),Y[,3])
fit_BayesB = bWGR::BayesB(Y[,2],wheat.X); cor(fit_BayesB$hat,Y[,3])
fit_emBayesA = bWGR::emBA(Y[,2],wheat.X); cor(fit_emBayesA$hat,Y[,3])
```

## Machine learning methods

Tree regression and classifiers

![](https://s3.ap-south-1.amazonaws.com/techleer/113.png)

## Machine learning methods

```{r,echo=TRUE,fig.width=7,fig.height=3.5,fig.align="center"}
fit_tree = party::ctree(y~.,data.frame(y=Y[,2],wheat.X)); plot(fit_tree)
cor(c(fit_tree@predict_response()),Y[,3])
```

## Machine learning methods

```{r,echo=TRUE,fig.width=3,fig.height=3,fig.align="center"}
fit_rf = ranger::ranger(y~.,data.frame(y=Y[,2],wheat.X))
plot(fit_rf$predictions,Y[,3],xlab='RF predictions from E2',ylab='Yield E3',pch=20)
cor(fit_rf$predictions,Y[,3])
```

## Genotype-Environment interactions

![](https://epi.grants.cancer.gov/i/hands_holding_two_puzzle_pieces.jpg)

## Genotype-Environmnet interactions

```{r,echo=TRUE}
y=as.vector(wheat.Y); Z=wheat.X; Zge=as.matrix(Matrix::bdiag(Z,Z,Z,Z))
#
fit_g = bWGR::BayesRR(rowMeans(wheat.Y),Z)
fit_ge = bWGR::BayesRR(y,Zge)
fit_gge = bWGR::BayesRR2(y,rbind(Z,Z,Z,Z),Zge)
#
fit_g$h2
fit_ge$h2
fit_gge$h2
```

## Genotype-Environmnet interactions

```{r,echo=TRUE,echo=FALSE,fig.width=8,fig.height=4,fig.align="center"}
require(Matrix,quietly = TRUE)
image(bdiag(Z[1:30,1:70],Z[1:30,1:70],Z[1:30,1:70],Z[1:30,1:70]),main="GxE design matrix: Example of 4 environments, 30 individuals, 70 SNPs")
```

## Genotype-Environmnet interactions

```{r,echo=TRUE,fig.width=4,fig.height=4,fig.align="center"}
GE1=matrix(fit_ge$hat,ncol=4); GE2=matrix(fit_ge$hat,ncol=4)
plot(data.frame(G=fit_g$hat,GE=rowMeans(GE1),G_and_GE=rowMeans(GE2)),main='GEBV across E')
```

## Genotype-Environmnet interactions

```{r,echo=TRUE,fig.width=8,fig.height=3.5,fig.align="center"}
par(mfrow=c(1,3))
plot(fit_g$hat,rowMeans(Y),main=round(cor(fit_g$hat,rowMeans(Y)),4),xlab='G',ylab='Obs',pch=20)
plot(c(GE1),y,main=round(cor(c(GE1),y),4),xlab='GE model',ylab='Observed',pch=20)
plot(c(GE2),y,main=round(cor(c(GE2),y),4),xlab='G+GE model',ylab='Observed',pch=20)
```

# Break
# Module 4 - Signal detection


<style>
  .col2 {
    columns: 2 200px;
    -webkit-columns: 2 200px;
    -moz-columns: 2 200px;
  }
</style>

## Outline

<div class="col2">
- Test statistics
- Allele coding
- Power & resolution
- Linkage mapping
- LD mapping
- Structure
- Imputation
- GLM
- MLM
- WGR
- Rare-variants
- Validation studies
</div>

```{r,echo=F}
require(NAM,quietly = T)
```


## Test statistics

- Testing associations are as simple as t-test and ANOVA

```{r,echo=F,fig.width=8,fig.height=4,fig.align="center"}
par(mfrow=c(1,2))
set.seed(1)
geno = factor(sort(rep(c(c('AA','Aa','aa')),100)))
pheno = c( rnorm(100,22,4), rnorm(100,24,5), rnorm(100,26,4) )
ppheno = c(pheno[1:200],pheno[101:300])
allele = factor(sort(rep(c(c('A','a')),200)))
boxplot(ppheno~allele,col=rainbow(5)[1:2],ylab='Phenotype',main='Allele frequencies (t-test)')
boxplot(pheno~geno,col=rainbow(5)[3:5],ylab='Phenotype',main='Genotypic frequencies (ANCOVA)')
abline(lm(pheno~as.numeric(geno)),lwd=2,col=2,lty=4)
```

## Test statistics

- A more generalized framework: Likelihood test

$$LRT = L_0 / L_1 = -2(logL_1 - logL_0)$$

For the model:

$$y=Xb+Zu+e\\ y\sim N(Xb,V)$$

REML function is given by:

$$L(\sigma^2_u,\sigma^2_e)=-0.5( ln|V|+ln|X'V^{-1}X|+y'Py)$$

Where $V=ZKZ'\sigma^2_u+I\sigma^2_e$ and $y'Py=y'e$


## Allele coding

Types of allele coding

1. Add. (1 df): {-1,0,1} or {0,1,2} - **Very popular** (Lines, GCA)
2. Dom. (1 df): {0,1,0} - **Popular** (Trees, clonals and Hybrids)
3. Jointly A+D (2 df): Popular on QTL mapping in F2s
4. Complete dominance (1 df): {0,0,1} or {0,1,1} - **Very unusual**
5. Interactions (X df): *Marker x Factor* (epistasis and GxE)

## Power and resolution

**Power**

- Key: Number of individuals & allele frequency
- More DF = lower power
- Multiple testing: Bonferroni and FDR
- Tradeoff: Power vs false positives

**Resolution**

- Genotyping density
- LD blocks
- Recombination

## Power: Variance of X

```{r,echo=F,fig.width=8,fig.height=5,fig.align="center"}
curve(2*x*(1-x),xlab='p',ylab='Var(x)',ylim=c(0,1.1),col=2,lwd=2,main='Marker variance')
curve( 4*x*(1-x), ylim=c(0,.6),col=4,lwd=2, add=T)
#
legend('topright',legend=c('HW (2pq)','Inbred (4pq)'),col=c(2,4),lty=1,lwd=2,bty='n')
#
text(0.5,0.55,'Unselected F2',col=2)
text(0.5,1.05,'Unselected DH / RIL',col=4)
lines(x=c(.5,.5),y=c(.5,1),col=c(2,4),pch=20,lwd=5,type='p')
#
arrows(x0=.5,x1=.5,y0=0,y1=.2,code = 1)
text(0.5,0.25,'p = q',cex=1.5)
```

##  Beavis effect: 1000 is just OK

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/Beavis_effect.png)

## Multiple testing:

GWAS tests $m$ hypothesis:

- No correction: $\alpha = 0.05$
- Bonferroni: $\alpha = 0.05/m$
- FDR (25\%): $\alpha = 0.05/(m\times0.75)$

```{r,echo=FALSE,fig.width=5,fig.height=3,fig.align="center"}
par(mar=c(1,1,1,1))
data(tpod); h=y+emRR(y,gen)$hat; fit = emGWA(h, gen);
CHR = c(); for(i in 1:20) CHR=c(CHR,rep(i,chr[i]))
plot(fit$PVAL*2,pch=16,col=CHR,ylim=c(0,5),xaxt='n',yaxt='n')
abline(h=c(1.2,3.8,3.3),lty=2,col=1:3) 
legend('topleft',c('No correction','Bonferroni','FDR 25%'),col=1:3,lty=2)
```



## Linkage mapping

- Generally on experimental pops (F2, DH, RIL, BC)
- Based on single-marker analysis or interval mapping
- Recombination rates would increase **power**

![](https://www.jax.org/-/media/jaxweb/images/news-and-insights/articles/graphics/10-2004-lodvchrome.jpg)

## LD mapping (or association mapping)

- Use of historical LD between marker and QTL
- AM allowed studies on random panels
- Dense SNP panels would increase **resolution**

![](https://www.researchgate.net/profile/Charlotte_Acharya/publication/237198966/figure/fig11/AS:213410093506584@1427892339496/Genome-wide-association-study-GWAS-for-growing-degree-days-to-silking-GWAS-for-growing.png)

## Structure

1. Confounding associations with sub-populations
2. Major limitation of association mapping
3. Structure: *PCs*, *clusters*, *subpopulation* (eg. race)

```{r,echo=F,fig.width=8,fig.height=4,fig.align="center"}
pc = svd(gen,2,2)
ds = hclust(dist(gen),method = 'ward.D2')
k = cutree(ds,4)
#
par(mfrow = c(1,2),mar = c(1,1,3,0))
plot(ds,labels = F,axes=F); abline(h=50,lty=3,col=2)
plot(pc$u,pch=20,main='Principal Components',col=k,axes = F)
abline(h=0,v=-.07,lty=2,col=4)
```

## Structure

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/PC_Europe.png)

## Imputation

Less missing values = more obs. = more detection power

- Markov models: Based on flanking markers
- Random forest: Multiple decision trees capture LD
- kNN & Projections: Fill with similar haplotypes

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/Impute.jpg)

## GLM (generalized linear models)

- Full model ($L_1$): $$y = Xb + m_ja + e$$
- Null model ($L_0$): $$y = Xb + e$$

1. **Advantage**: Fast, not restricted to Gaussian traits
2. Popular methodology on human genetic studies
3. $Xb$ includes (1) environment, (2) structure and (3) covariates


## MLM (mixed linear models)

- Full model ($L_1$): $$y = Xb + Zu + m_ja + e$$
- Null model ($L_0$): $$y = Xb + Zu + e$$

1. The famous "**Q+K model**"
2. **Advantage**: Better control of false positives, no need for PCs
3. Polygenic effect ($u$) assumes $u\sim N(0,K\sigma^2_u)$
4. Faster if we don't reestimate $\lambda = \sigma^2_e/\sigma^2_u$ for each SNP


## cMLM (compressed MLM)

1. Uses the same base model as MLM
2. **Advantage**: Faster than MLM
3. Based on clustered individuals:
- $Z$ is indicates the subgroup
- $K$ is the relationship among subgroup
- Often needs PCs to complement $K$

## WGR (whole-genome regression)

1. Tests all markers at once
2. **Advantage**: No double-fitting, no PCs, no Bonferroni
- Model (BayesB, BayesC, SSVS): $$y = Xb + Ma + e$$ 
- Marker effects are from a mixture of distributions

$a_j \sim Binomial$ with $p(\pi) = 0$ and $p(1-\pi) = a_j$

## WGR (whole-genome regression)

```{r,echo=F,fig.width=8,fig.height=5,fig.align="center"}
mlm = emGWA(y,gen)
BB = emBB(y,gen,Pi=0.5)
par(mfrow=c(1,2))
plot(mlm$PVAL,main='MLM',ylab='-log(p)',xlab='SNP',type='h')
plot(-log(1-BB$d),main='BayesB',ylab='-log(p)',xlab='SNP',type='h')
```

## Rare variants

1. Screen a set ($s$) of low MAF markers on NGS data
2. **Advantage**: Detect signals from low power SNPs
3. Applied to uncommon diseases (not seen in plant breeding)
4. Two possible model
- Full model 1 ($L_1$): $y = Xb + M_sa + e$
- Full model 2 ($L_2$): $y = Xb + PC_1(M_s) + e$
- Null model ($L_0$): $y = Xb + e$

Test either $LR(L_1,L_0)$ or $LR(L_2,L_0)$

## Validation studies

- QTLs detected with 3 methods, across 3 mapping pops
- Validations made on 3 unrelated populations

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/QTL_validation.png)


# Break
# Module 5 - Association analysis
## Prelude: Data & Structure

```{r,echo=FALSE}
require(NAM,quietly=TRUE)
```

## Getting some data

Example dataset from the *SoyNAM* package. We are querying two of the forty biparental families with a shared parental IA3023, grown in 18 environment.

```{r,echo=TRUE}
Data = SoyNAM::BLUP(trait = 'yield', family = 2:3)
```

## Genomic relationship matrix

```{r,echo=TRUE}
y = Data$Phen
M = Data$Gen
#
Z = apply(M,2,function(snp) snp-mean(snp))
ZZ = tcrossprod(Z)
Sum2pq = sum(apply(M,2,function(snp){p=mean(snp)/2; return(2*p*(1-p))}))
G = ZZ/Sum2pq
```

Kernel commonly deployed, referred in VanRaden (2008)

$$G=\frac{(M-P)(M-p)'}{2\sum^{J}_{j=1}{p_j(1-p_j)}}$$

## Genomic relationship matrix

```{r,echo=TRUE,fig.width=5,fig.height=5,fig.align="center"}
image(G[,241:1], main='GRM heatmap',xaxt='n',yaxt='n')
```


## Structure parameters (1) PCs

```{r,echo=TRUE,fig.width=6,fig.height=4,fig.align="center"}
Spectral = eigen(G,symmetric = TRUE)
PCs = Spectral$vectors[,1:5]
plot(PCs,xlab='PC 1',ylab='PC 2',main='Population on eigen spaces',col=Data$Fam,pch=20)
```

## Structure parameters (2) Clusters

```{r,echo=TRUE,fig.width=6,fig.height=4,fig.align="center"}
GeneticDistance = Gdist(M,method=6)
Tree = hclust(GeneticDistance,method = 'ward.D2')
plot(Tree,labels = FALSE)
Clst = factor(cutree(Tree,k=2))
```

## Single marker analysis

## GLM (1) - No structure

```{r,echo=TRUE}
Marker = M[,117]
#
fit = lm( y ~ Marker )
anova( fit )
-log(anova(fit)$`Pr(>F)`[1],base = 10)
```

## GLM (2) - Principal Components

```{r,echo=TRUE}
reduced_model = lm( y ~ PCs )
full_model = lm( y ~ PCs + Marker )
anova( reduced_model, full_model )
-log((anova( reduced_model, full_model ))$`Pr(>F)`[2],base = 10)
```

## GLM (3) - Population Clusters

```{r,echo=TRUE}
reduced_model = lm( y ~ Clst )
full_model = lm( y ~ Clst + Marker )
anova( reduced_model, full_model )
-log( anova(reduced_model,full_model)$`Pr(>F)`[2],base = 10)
```

## MLM - K+Q model

```{r,echo=TRUE}
Q = model.matrix(~Clst)
reduced_model = reml( y=y, X=Q, K=G)
full_model = reml( y=y, X=cbind(Q, Marker), K=G)
LRT = -2*(full_model$loglik - reduced_model$loglik)
-log(pchisq(LRT,1,lower.tail=FALSE),base=10)
```

## Whole genome screening

## DIY (example with GLM)

```{r,echo=TRUE,fig.width=8,fig.height=3.5,fig.align="center"}
reduced_model = lm( y ~ Clst )
glm_pval = apply(M,2,function(Marker){
  pval = anova(reduced_model, lm(y~Clst+Marker) )$`Pr(>F)`[2]
  return(-log(pval,base = 10))})
plot(glm_pval,pch=20,xlab='SNP',main='My first GLM GWAS')
```

## Using CRAN implementations

NAM random model: $y=\mu+Marker \times Pop+Zu+e$ 

```{r,echo=TRUE}
fit_gwa = gwas3(y=y, gen=M, fam=c(Clst), chr=Data$Chrom)
```

## Manhattan plot

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
plot(fit_gwa, pch=20, main = "My first MLM GWAS")
```

## Yet another R implementations

```{r,echo=TRUE}
require(rrBLUP,quietly = TRUE); COL = fit_gwa$MAP[,1]%%2+1 # Color chromosomes
geno=data.frame(colnames(M),fit_gwa$MAP[,1:2],t(M-1),row.names=NULL)
pheno=data.frame(line=colnames(geno)[-c(1:3)],Pheno=y,Clst,row.names=NULL)
fit_another_gwa=GWAS(pheno,geno,fixed='Clst',plot=FALSE)
```

```{r,echo=TRUE,echo=FALSE,fig.width=6,fig.height=2.5,fig.align="center"}
mlm_pval=fit_another_gwa$Pheno; mlm_pval[mlm_pval==0]=NA
par(mar=c(0,0,0,0))
plot(mlm_pval,col=COL+3,pch=20);
```


## Comparing results

```{r,echo=TRUE,fig.width=4,fig.height=4,fig.align="center"}
mlm_pval=fit_another_gwa$Pheno; mlm_pval[mlm_pval==0]=NA
plot(glm_pval,mlm_pval,xlab='GLM',ylab='MLM',main='Compare')
```

## Power analysis - QQ plot

```{r,echo=TRUE,fig.width=9,fig.height=3,fig.align="center"}
nam_pval = fit_gwa$PolyTest$pval
par(mfrow=c(1,3))
qqman::qq(glm_pval,main='GLM')
qqman::qq(mlm_pval,main='MLM')
qqman::qq(nam_pval,main='RLM')
```

## Multiple testing

## Multiple testing

*Wikipedia:* In statistics, the **multiple comparisons**, **multiplicity** or **multiple testing** problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values.In certain fields it is known as the look-elsewhere effect: *The more inferences are made, the more likely erroneous inferences are to occur.* Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a **stricter significance threshold** for individual comparisons, so as to compensate for the number of inferences being made.

## Baseline - No correction

Base significance threshold: $\alpha=0.05/m$

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
plot(fit_gwa, alpha=0.05, main = "No correction")
```


## Multiple testing correction

Bonferroni: $\alpha=0.05/m$

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
plot(fit_gwa, alpha=0.05/ncol(M), main = "Bonferroni correction")
```

## False-Discovery Rate

Benjamini-Hochberg FDR: $\alpha=\frac{0.05}{m\times (1-FDR)}$

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
plot(fit_gwa, alpha=0.05/(ncol(M)*.75), main = "FDR 25%")
```

## False-Discovery Rate

Unique segments based on Eigenvalues: $m^* = D > 1$

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
m_star = sum(Spectral$values>1)
plot(fit_gwa, alpha=0.05/m_star, main="Bonferroni on unique segments")
```

## Multi-loci analysis

## Whole genome regression

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
fit_wgr = bWGR::BayesDpi(y=y,X=M,it=3000); par(mfrow=c(1,2));
plot(fit_wgr$PVAL,col=COL,pch=20,ylab='-log(p)',main='GWA')
plot(fit_wgr$b,col=COL,pch=20,ylab='Marker effect',main='GWP')
plot(fit_wgr$hat,y,pch=20)
```

## WGR - No need for multiple testing

```{r,echo=TRUE,fig.width=8,fig.height=4,fig.align="center"}
thr_none = -log(pchisq(qchisq(1-0.05/ncol(M),1),1,lower.tail=FALSE),base=10)
thr_bonf = -log(pchisq(qchisq(1-0.05,1),1,lower.tail=FALSE),base=10)
par(mfrow=c(1,2)); plot(fit_gwa,alpha=NULL,main="MLM",pch=20); abline(h=thr_none,col=3)
plot(fit_wgr$PVAL,col=COL,ylab='-log(p)',main="WGR",pch=20); abline(h=thr_bonf,col=3)
```

## Approaches are complementary

```{r,echo=TRUE,echo=FALSE,fig.width=10,fig.height=5,fig.align="center"}
thr_fdr = -log(pchisq(qchisq(1-0.05/m_star,1),1,lower.tail=FALSE),base=10)
par(mfrow=c(2,2),mar=c(0,1,1.5,1));
plot(glm_pval,main="GLM",col=COL+2,pch=20,,xaxt='n',yaxt='n');
abline(h=thr_none,col=3); abline(h=thr_bonf,lty=2,col=3); abline(h=thr_fdr,lty=3,col=3);
plot(mlm_pval,alpha=NULL,main="MLM",col=COL+1,pch=20,xaxt='n',yaxt='n');
abline(h=thr_none,col=3); abline(h=thr_bonf,lty=2,col=3); abline(h=thr_fdr,lty=3,col=3);
plot(fit_gwa,alpha=NULL,main="RLM",pch=20,xaxt='n',yaxt='n');
abline(h=thr_none,col=3); abline(h=thr_bonf,lty=2,col=3); abline(h=thr_fdr,lty=3,col=3);
plot(fit_wgr$PVAL,col=COL,ylab='-log(p)',main="WGR",pch=20,xaxt='n',yaxt='n');
abline(h=thr_none,col=3); abline(h=thr_bonf,lty=2,col=3); abline(h=thr_fdr,lty=3,col=3);
```

## Random forest

```{r,echo=TRUE,fig.width=10,fig.height=4,fig.align="center"}
fit_rf = ranger::ranger(y~.,data= data.frame(y=y,M),importance='impurity')
plot(fit_rf$variable.importance,ylab='Importance',main='Random Forest',col=COL+7,pch=20)
```

## Thanks!

Thanks!

+ e-mail: xaviera@purdue.edu or 
+ other resources: https://alenxav.wixsite.com/home
+ material: https://github.com/alenxav/Lectures/tree/master/Purdue_MLM

# Break