---
title: "Lecture 1 - A brief introduction to mixed models"
author: "Alencar Xavier, Gota Morota"
date: "October 26, 2018"
widescreen: true
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Instructors

Alencar Xavier

- Quantitative Geneticist, Corteva Agrisciences
- Adjunct professor, Purdue University
- http://alenxav.wixsite.com/home/

Gota Morota

- Assistant professor, Virginia Tech
- http://morotalab.org/

## Outline

Part 1: Concepts

- History of mixed models
- Mixed models in plant breeding
- Fixed and random terms
- Model notation
- Variance decomposition

Part 2: Applications

- Selection models
- Practical examples
- Variance components
- Ridges and Kernels

# Part 1 - Concepts

## History of mixed models

*Francis Galton* - [1886](http://rspl.royalsocietypublishing.org/content/40/242-245/42): Regression and heritability

*Ronald Fisher* - [1918](https://www.jstor.org/stable/2331838): Infinitesimal model (**P = G + E**)

*Sewall Wright* - [1922](https://www.journals.uchicago.edu/doi/pdfplus/10.1086/279872): Genetic relationship

*Charles Henderson* - [1950](http://morotalab.org/literature/pdf/henderson1950.pdf), [1968](https://www.jstor.org/stable/2528457): BLUP using relationship 

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/geneticists.jpg)

## Mixed models in plant breeding

- *Heart and soul* of plant breeding [(Xavier et al 2017)](https://link.springer.com/article/10.1007/s00122-016-2750-y)
- Variance components and heritability
- Trait associations [(Gianola and Sorensen 2014)](http://www.genetics.org/content/167/3/1407)
- Estimation of genetic values [(Piepho et al 2008)](https://link.springer.com/article/10.1007/s10681-007-9449-8)
- Estimation of breeding values
- Prediction of unphenotyped lines [(de los Campos et al 2013)](http://www.genetics.org/content/early/2012/06/28/genetics.112.143313)
- Selection index
- Genome-wide association analysis [(Yang et al 2014)](https://www.nature.com/articles/ng.2876)
- All sorts of inference [(Robinson 1991)](https://projecteuclid.org/download/pdf_1/euclid.ss/1177011926)

## Fixed and random terms

**Fixed effect**

- Assumed to be invariable (often you cannot recollect the data)
- Inferences are made upon the parameters
- Results can not be extrapolated to other datasets
- Example: Overall mean and environmental effects

**Random effects**

- You may not have all the levels available
- Inference are made on variance components
- Prior assumption: coefficients are normally distributed
- Results can not be extrapolated to other datasets
- Example: Genetic effects

## Let's unleash the beast

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/unleash.jpg)

## Model notation

- Linear model: $y=Xb+Zu+e$
- With variance: $y \sim N(Xb,ZKZ\sigma^{2}_{u}+I\sigma^{2}_{e})$

Assuming: $u \sim N(0,K\sigma^{2}_{u})$ and $e \sim N(0,I\sigma^{2}_{e})$

Henderson equation

$$
\left[\begin{array}{rr} X'X & Z'X \\ X'Z & Z'Z+\lambda K^{-1} \end{array}\right] 
\left[\begin{array}{r} b \\ u \end{array}\right]
=\left[\begin{array}{r} X'y \\ Z'y \end{array}\right]
$$
Summary:

- We know (data): $x=\{y,X,Z,K\}$
- We want (parameters): $\theta=\{b,u,\sigma^{2}_{a},\sigma^{2}_{e}\}$
- Estimation based on Gaussian likelihood: $L(x|\theta)$

## Model notation

- **y** = vector of observations (*n*)
- **X** = design matrix of fixed effects (*n* x *p*)
- **Z** = design (or incidence) matrix of random effects (*n* x *p*)
- **K** = random effect correlation matrix (*q* x *q*)
- **u** = vector of random effect coefficients (*q*)
- **b** = vector of fixed effect coefficients (*p*)
- **e** = vector of residuals (*n*)
- $\sigma^{2}_{a}$ = marker effect variance (1)
- $\sigma^{2}_{u}$ = random effect variance (1)
- $\sigma^{2}_{e}$ = residual variance (1)
- $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}$ (Regularization parameters) (1)

## Model notation

The mixed model can also be notated as follows

$$y=Wg+e$$

Solved as

$$[W'W+\Sigma] g = [W'y]$$

Where

$W=[X,Z]$

$g=[b,u]$

$\Sigma = \left[\begin{array}{rr} 0 & 0 \\ 0 & \lambda K^{-1} \end{array}\right]$

## Variance decomposition

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/VarComp.jpg)

# Part 2 - Applications

## Selection

1 *Genetic values*

- BLUPs or BLUEs from replicated trials
- Captures additive and non-additive genetics together

2 *Breeding values*

- Use pedigree information to create $K$
- Captures additive genetics (heritable)
- Trials not necessarily replicated

3 *Genomic Breeding values*

- Genotypic information replaces pedigree
- Any signal: additivity, dominance and epistasis

## Examples

- Example 1: Balanced data, no kinship
- Example 2: Balanced data, with kinship
- Example 3: Unbalanced data, with kinship
- Example 4: Balanced data, missing individual

## Example 1

Data:

```{r}
set.seed(1)
ex = data.frame(Env=factor(paste('E',c(1,1,1,1,2,2,2,2,3,3,3,3),sep='')),
                Gen=factor(paste('G',c(1,2,3,4,1,2,3,4,1,2,3,4),sep='')),
                Phe=round(rnorm(12,50,5)))
print(ex)
```

Model: $Phenotype = Environment_{(F)} + Genotype_{(R)}$

## Example 1

Design matrix $W$:

```{r}
X = model.matrix(~Env-1,ex)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(~Gen-1,ex))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex$Phe
print(W)
```

## Example 1

$W'W$:

```{r}
WW = crossprod(W)
print(WW)
```

## Example 1

Left-hand side ($W'W+\Sigma$):

```{r}
WW = crossprod(W)
Sig = diag(c(rep(0,3),rep(0.17,4)))
LHS = WW+Sig;
print(LHS)
```

Assuming independent individuals: $K=I$

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.64/9.56=0.17$

## Example 1

Right-hand side ($W'y$):

```{r}
RHS = crossprod(W,y)
print(RHS)
```

## Example 1

We can find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r}
g = solve(LHS,RHS)
print(round(g,2))
```

## Shrinkage

$BLUE=\frac{w'y}{w'w}=\frac{sum}{n}$ = *simple average*

$BLUP=\frac{w'y}{w'w+\lambda}=\frac{sum}{n+\lambda}$ = *biased average* = $BLUE\times h^2$

![](https://raw.githubusercontent.com/alenxav/miscellaneous/master/Shrinkage.jpg)

**Note**:

- More observations = less shrinkage
- Higher heritability = less shrinkage: $\lambda = \frac{h^{2}-1}{h^2}$


## Example 2

If we know the relationship among individuals:

```{r}
set.seed(1)
K = crossprod(matrix(rpois(16,3),4,4))
K = round(K/max(K),2)
diag(K) = 1
colnames(K) = rownames(K) = colnames(Z)
print(K)
```

## Example 2

Then we estimate $\lambda K^{-1}$

```{r}
iK_Lmb = round(solve(K)*0.09,2)
print(iK_Lmb)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.64/17.70=0.09$

## Example 2

And the left-hand side becomes

```{r}
LHS2 = WW;
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

## Example 2

We can find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

Genetic coefficients shrink more: Var(A) < Var(G)

## Example 3

What if we have missing data?

```{r}
ex2 = ex
ex2$Phe[c(3,8)] = NA
print(ex2)
```

## Example 3

Rows of missing points are removed

```{r}
X = model.matrix(Phe~Env-1,ex2)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(Phe~Gen-1,ex2))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex2$Phe[-c(3,8)]
print(W)
```

## Example 3

$W'W$:

```{r}
WW = crossprod(W)
print(WW)
```

## Example 3

Left-hand side ($W'W+\Sigma$):

```{r}
LHS2 = WW;
iK_Lmb = round(solve(K)*0.06,2)
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.21/19.61=0.06$

## Example 3 

Right-hand side ($W'y$):

```{r}
RHS = crossprod(W,y)
print(RHS)
```

## Example 3

Find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

## Example 4

What if we are missing data from a individual?

```{r}
ex2 = ex
ex2$Phe[c(1,5,9)] = NA
print(ex2)
```

## Example 4

Rows of missing points are removed

```{r}
X = model.matrix(Phe~Env-1,ex2)
attr(X, "assign") = attr(X, "contrasts") = NULL;
Z = data.matrix(model.matrix(Phe~Gen-1,ex2))
attr(Z, "assign") = attr(Z, "contrasts") = NULL;
W = cbind(X,Z)
y = ex2$Phe[-c(1,5,9)]
print(W)
```

## Example 4

$W'W$:

```{r}
WW = crossprod(W)
print(WW)
```

## Example 4

Left-hand side ($W'W+\Sigma$):

```{r}
LHS2 = WW;
iK_Lmb = round(solve(K)*0.08,2)
LHS2[4:7,4:7] = LHS2[4:7,4:7]+iK_Lmb 
print(LHS2)
```

Regularization: $\lambda=\sigma^{2}_{e}/\sigma^{2}_{u}=1.79/22.78=0.08$

## Example 4

Right-hand side ($W'y$):

```{r}
RHS = crossprod(W,y)
print(RHS)
```

## Example 4

Find coefficients through least-square solution

$g=(LHS)^{-1}(RHS)=(W'W+\Sigma)^{-1}W'y$

```{r}
g2 = solve(LHS2,RHS)
print(round(g2,2))
```

## Variance components

Expectation-Maximization REML [(1977)](https://www.jstor.org/stable/2286796)

$\sigma^{2}_{u} = \frac{u'K^{-1}u}{q-\lambda tr(K^{-1}C^{22})}$ and $\sigma^{2}_{e} = \frac{e'y}{n-p}$

Bayesian Gibbs Sampling [(1993)](https://gsejournal.biomedcentral.com/track/pdf/10.1186/1297-9686-25-1-41)

$\sigma^{2}_{u} = \frac{u'K^{-1}u+S_u\nu_u}{\chi^2(q+\nu_u)}$ and $\sigma^{2}_{e} = \frac{e'e+S_e\nu_e}{\chi^2(n+\nu_e)}$

Predicted Residual Error Sum of Squares (PRESS) [(2017)](http://www.g3journal.org/content/7/3/895)

- $\lambda = argmin(\sum{e^2_i/(1-h_{ii})^2})$
- Where $H=(I+K\lambda)^{-1}$ and $e=y-\mu-Hy$

## Ridges and Kernels

Kernel methods:

- Genetic signal is captured by the relationship matrix $K$
- Random effect coefficients are the **breeding values** (BV)
- Efficient to compute BV when $markers \gg individuals$
- Easy use and combine pedigree, markers and interactions

Ridge methods:

- Genetic signal is captured by the design matrix $M$
- Random effect coefficients are the **marker effects**
- Easy way to make predictions of unobserved individuals
- Enables to visualize where the QTLs are in the genome

## Ridges and Kernels

Kernel

$y=Xb+Zu+e$, $u\sim N(0,K\sigma^2_u)$

Ridge

$y=Xb+Ma+e$, $a\sim N(0,I\sigma^2_a)$

Where

- $M$ is the genotypic matrix, $m_{ij}=\{0,1,2\}$
- $K=\alpha MM'$
- $u=Ma$
- $\sigma^2_a=\alpha\sigma^2_u$

## Ridges and Kernels

Kernel model

$$
\left[\begin{array}{rr} X'X & Z'X \\ X'Z & Z'Z+K^{-1}(\sigma^2_e/\sigma^2_u) \end{array}\right] 
\left[\begin{array}{r} b \\ u \end{array}\right]
=\left[\begin{array}{r} X'y \\ Z'y \end{array}\right]
$$

Ridge model

$$
\left[\begin{array}{rr} X'X & M'X \\ X'M & M'M+I^{-1}(\sigma^2_e/\sigma^2_a) \end{array}\right] 
\left[\begin{array}{r} b \\ a \end{array}\right]
=\left[\begin{array}{r} X'y \\ M'y \end{array}\right]
$$

Both models capture same genetic signal [(de los Campos 2015)](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1005048)

## Ridges and Kernels

```{r}
require(NAM,quietly = TRUE)
data(tpod)
M = gen
par(mar=c(4,4,0,0))
```

```{r,echo=TRUE,fig.width=5,fig.height=4,fig.align="center"}
K = tcrossprod(M)/ncol(M)
GBLUP = reml(y=y,K=K); Kernel_GEBV = GBLUP$EBV
RRBLUP = reml(y=y,Z=M); Ridge_GEBV = M%*%RRBLUP$EBV
plot(Kernel_GEBV,Ridge_GEBV, main='Comparing results')
```

# Break
